{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "688acde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install pystow==0.4.9 # the most recent version 0.5.0 breaks pykeen, so we use the old version\n",
    "!pip install git+https://github.com/pykeen/pykeen.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b3f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dda263dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pykeen\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Load processed data\n",
    "os.chdir(r\"C:\\Users\\maxmo\\Dropbox\\GDS\\graph_data_analytics\\2025-wt-a2-what-to-watch\")\n",
    "\n",
    "genome_scores = pd.read_csv(r'processed_data\\genome_scores_processed_small.csv')\n",
    "movies_processed = pd.read_csv(r'processed_data\\movies_processed.csv')\n",
    "ratings_train = pd.read_csv(r'processed_data\\ratings_train_small.csv')\n",
    "movies_test_cold_start = pd.read_csv(r'processed_data\\movies_test_cold_start.csv')\n",
    "ratings_test_cold_start = pd.read_csv(r'processed_data\\ratings_test_cold_start.csv')\n",
    "ratings_test = pd.read_csv(r'processed_data\\ratings_test_small.csv')\n",
    "ratings_val_cold_start = pd.read_csv(r'processed_data\\ratings_val_cold_start.csv')\n",
    "ratings_val = pd.read_csv(r'processed_data\\ratings_val_small.csv')\n",
    "# Map user and movie IDs to indices\n",
    "user_ids = ratings_train['userId'].unique()\n",
    "movie_ids = movies_processed['movieId'].unique()\n",
    "user2id = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "movie2id = {movie_id: idx + len(user_ids) for idx, movie_id in enumerate(movie_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76035142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>positive_rating</th>\n",
       "      <th>negative_rating</th>\n",
       "      <th>year_month</th>\n",
       "      <th>last_rating_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>750</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-02-24 03:43:07</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>2016-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>109487</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2016-02-24 03:43:16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>2016-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>293</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-02-24 03:43:18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>2016-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>99114</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2016-02-24 03:43:29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>2016-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>68157</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-02-24 03:43:34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02</td>\n",
       "      <td>2016-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29021</th>\n",
       "      <td>162536</td>\n",
       "      <td>105844</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2019-10-28 10:45:26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10</td>\n",
       "      <td>2019-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29022</th>\n",
       "      <td>162536</td>\n",
       "      <td>121231</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2019-10-28 10:45:38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10</td>\n",
       "      <td>2019-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29023</th>\n",
       "      <td>162536</td>\n",
       "      <td>1208</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-10-29 23:20:36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10</td>\n",
       "      <td>2019-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29024</th>\n",
       "      <td>162536</td>\n",
       "      <td>1222</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-10-29 23:20:55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10</td>\n",
       "      <td>2019-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29025</th>\n",
       "      <td>162536</td>\n",
       "      <td>1704</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2019-11-10 11:41:44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11</td>\n",
       "      <td>2019-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29026 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating            timestamp  positive_rating  \\\n",
       "0          21      750     4.0  2016-02-24 03:43:07                1   \n",
       "1          21   109487     5.0  2016-02-24 03:43:16                1   \n",
       "2          21      293     4.0  2016-02-24 03:43:18                1   \n",
       "3          21    99114     4.5  2016-02-24 03:43:29                1   \n",
       "4          21    68157     4.0  2016-02-24 03:43:34                1   \n",
       "...       ...      ...     ...                  ...              ...   \n",
       "29021  162536   105844     4.5  2019-10-28 10:45:26                1   \n",
       "29022  162536   121231     4.5  2019-10-28 10:45:38                1   \n",
       "29023  162536     1208     5.0  2019-10-29 23:20:36                1   \n",
       "29024  162536     1222     4.0  2019-10-29 23:20:55                1   \n",
       "29025  162536     1704     4.5  2019-11-10 11:41:44                1   \n",
       "\n",
       "       negative_rating year_month last_rating_year  \n",
       "0                    0    2016-02          2016-02  \n",
       "1                    0    2016-02          2016-02  \n",
       "2                    0    2016-02          2016-02  \n",
       "3                    0    2016-02          2016-02  \n",
       "4                    0    2016-02          2016-02  \n",
       "...                ...        ...              ...  \n",
       "29021                0    2019-10          2019-11  \n",
       "29022                0    2019-10          2019-11  \n",
       "29023                0    2019-10          2019-11  \n",
       "29024                0    2019-10          2019-11  \n",
       "29025                0    2019-11          2019-11  \n",
       "\n",
       "[29026 rows x 8 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "03696aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.models import RESCAL, TransE, DistMult, TransH\n",
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "from pykeen.triples import TriplesFactory\n",
    "\n",
    "def create_triples_factory(ratings, user2id, movie2id):\n",
    "    \"\"\"Create a TriplesFactory from a ratings DataFrame.\"\"\"\n",
    "    triples = []\n",
    "    for _, row in ratings.iterrows():\n",
    "        user_idx = user2id[row['userId']]\n",
    "        movie_idx = movie2id[row['movieId']]\n",
    "        if row['rating'] >= 4.0:\n",
    "            edge_type = 'liked'\n",
    "        elif row['rating'] < 2.5:\n",
    "            edge_type = 'disliked'\n",
    "        else:\n",
    "            continue  # Skip neutral ratings\n",
    "        triples.append((user_idx, edge_type, movie_idx))\n",
    "    triples_np = np.array(triples, dtype=str)\n",
    "    return TriplesFactory.from_labeled_triples(\n",
    "        triples=triples_np,\n",
    "        create_inverse_triples=False\n",
    "    )\n",
    "\n",
    "# Create TriplesFactory for training, validation, and testing sets\n",
    "training_set = create_triples_factory(ratings_train, user2id, movie2id)\n",
    "validation_set = create_triples_factory(ratings_val, user2id, movie2id)\n",
    "testing_set = create_triples_factory(ratings_test, user2id, movie2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9884389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pykeen.triples.triples_factory.TriplesFactory'>\n",
      "<class 'pykeen.triples.triples_factory.TriplesFactory'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_set))\n",
    "print(type(validation_set))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50134ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_users = set(ratings_train['userId'].unique())\n",
    "seen_movies = set(ratings_train['movieId'].unique())\n",
    "ratings_test_filtered = ratings_test[\n",
    "    (ratings_test['userId'].isin(seen_users)) &\n",
    "    (ratings_test['movieId'].isin(seen_movies))\n",
    "]\n",
    "\n",
    "ratings_test_filtered['user_idx'] = ratings_test_filtered['userId'].map(user2id)\n",
    "ratings_test_filtered['movie_idx'] = ratings_test_filtered['movieId'].map(movie2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "75b6edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.stoppers.early_stopping:Inferred checkpoint path for best model weights: C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-80a6aa18-c52a-41b6-8fd1-ec307d0b89a0.pt\n",
      "INFO:pykeen.stoppers.early_stopping:Inferred checkpoint path for best model weights: C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-80a6aa18-c52a-41b6-8fd1-ec307d0b89a0.pt\n",
      "c:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Training epochs on cpu:  20%|██        | 4/20 [00:15<00:45,  2.84s/epoch, loss=0.465, prev_loss=0.474]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 47.30s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 47.30s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 5: 0.006325679204965573. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-80a6aa18-c52a-41b6-8fd1-ec307d0b89a0.pt\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 5: 0.006325679204965573. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-80a6aa18-c52a-41b6-8fd1-ec307d0b89a0.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 5.\n",
      "Training epochs on cpu:  25%|██▌       | 5/20 [01:02<05:03, 20.26s/epoch, loss=0.465, prev_loss=0.474]INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 5.\n",
      "Training epochs on cpu:  45%|████▌     | 9/20 [01:19<01:16,  6.98s/epoch, loss=0.447, prev_loss=0.452]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 46.11s seconds\n",
      "Training epochs on cpu:  50%|█████     | 10/20 [02:05<03:19, 19.97s/epoch, loss=0.447, prev_loss=0.452]INFO:pykeen.evaluation.evaluator:Evaluation took 46.11s seconds\n",
      "Training epochs on cpu:  70%|███████   | 14/20 [02:19<00:41,  6.88s/epoch, loss=0.444, prev_loss=0.443]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 47.41s seconds\n",
      "Training epochs on cpu:  75%|███████▌  | 15/20 [03:06<01:39, 19.93s/epoch, loss=0.444, prev_loss=0.443]INFO:pykeen.evaluation.evaluator:Evaluation took 47.41s seconds\n",
      "Training epochs on cpu:  95%|█████████▌| 19/20 [03:22<00:07,  7.09s/epoch, loss=0.437, prev_loss=0.439]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 50.76s seconds\n",
      "Training epochs on cpu: 100%|██████████| 20/20 [04:13<00:00, 12.66s/epoch, loss=0.437, prev_loss=0.439]INFO:pykeen.evaluation.evaluator:Evaluation took 50.76s seconds\n",
      "Training epochs on cpu: 100%|██████████| 20/20 [04:13<00:00, 12.66s/epoch, loss=0.437, prev_loss=0.439]\n",
      "\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 29.0k/29.0k [00:51<00:00, 568triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 51.91s seconds\n",
      "Evaluating on cpu: 100%|██████████| 29.0k/29.0k [00:51<00:00, 568triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 51.91s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 37min 46s\n",
      "Wall time: 5min 8s\n",
      "Parser   : 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.manual_seed(seed=42)\n",
    "np.random.seed(seed=42)\n",
    "transH_train = pipeline(\n",
    "    model=TransH,\n",
    "    model_kwargs={\"random_seed\": 0},\n",
    "    training=training_set,\n",
    "    dimensions=64,\n",
    "    epochs=20,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    stopper=\"early\",\n",
    "    stopper_kwargs={\n",
    "        \"frequency\": 5,\n",
    "        \"patience\": 10,\n",
    "        \"relative_delta\": 0.0,\n",
    "    },\n",
    "    validation=validation_set,\n",
    "    testing=testing_set,\n",
    "    evaluator=RankBasedEvaluator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "616cd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.stoppers.early_stopping:Inferred checkpoint path for best model weights: C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-b2c506ef-2a8a-4e28-803d-6d4af937903d.pt\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.stoppers.early_stopping:Inferred checkpoint path for best model weights: C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-b2c506ef-2a8a-4e28-803d-6d4af937903d.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs on cpu:  20%|██        | 4/20 [00:21<01:09,  4.32s/epoch, loss=21.7, prev_loss=22.9]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 23.87s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 23.87s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 5: 0.006817488043957535. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-b2c506ef-2a8a-4e28-803d-6d4af937903d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 5.\n",
      "Training epochs on cpu:  25%|██▌       | 5/20 [00:45<03:11, 12.77s/epoch, loss=21.7, prev_loss=22.9]INFO:pykeen.stoppers.early_stopping:New best result at epoch 5: 0.006817488043957535. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-b2c506ef-2a8a-4e28-803d-6d4af937903d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 5.\n",
      "Training epochs on cpu:  45%|████▌     | 9/20 [01:05<01:03,  5.80s/epoch, loss=13.1, prev_loss=14.8]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 24.45s seconds\n",
      "Training epochs on cpu:  50%|█████     | 10/20 [01:29<02:07, 12.77s/epoch, loss=13.1, prev_loss=14.8]INFO:pykeen.evaluation.evaluator:Evaluation took 24.45s seconds\n",
      "Training epochs on cpu:  70%|███████   | 14/20 [01:50<00:37,  6.21s/epoch, loss=5.26, prev_loss=6.77]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 24.08s seconds\n",
      "Training epochs on cpu:  75%|███████▌  | 15/20 [02:14<01:04, 12.87s/epoch, loss=5.26, prev_loss=6.77]INFO:pykeen.evaluation.evaluator:Evaluation took 24.08s seconds\n",
      "Training epochs on cpu:  95%|█████████▌| 19/20 [02:36<00:06,  6.45s/epoch, loss=1.16, prev_loss=1.41]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 23.99s seconds\n",
      "Training epochs on cpu: 100%|██████████| 20/20 [03:00<00:00,  9.00s/epoch, loss=1.16, prev_loss=1.41]INFO:pykeen.evaluation.evaluator:Evaluation took 23.99s seconds\n",
      "Training epochs on cpu: 100%|██████████| 20/20 [03:00<00:00,  9.00s/epoch, loss=1.16, prev_loss=1.41]\n",
      "\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 29.0k/29.0k [00:28<00:00, 1.04ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 28.91s seconds\n",
      "Evaluating on cpu: 100%|██████████| 29.0k/29.0k [00:28<00:00, 1.04ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 28.91s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25min 59s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.manual_seed(seed=42)\n",
    "np.random.seed(seed=42)\n",
    "rescal_train = pipeline(\n",
    "    model=RESCAL,\n",
    "    model_kwargs={\"random_seed\": 0},\n",
    "    training=training_set,\n",
    "    dimensions=64,\n",
    "    epochs=20,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    stopper=\"early\",\n",
    "    stopper_kwargs={\n",
    "        \"frequency\": 5,\n",
    "        \"patience\": 10,\n",
    "        \"relative_delta\": 0.0,\n",
    "    },\n",
    "    validation=validation_set,\n",
    "    testing=testing_set,\n",
    "    evaluator=RankBasedEvaluator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e059c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.pipeline.api:Using device: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.stoppers.early_stopping:Inferred checkpoint path for best model weights: C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.stoppers.early_stopping:Inferred checkpoint path for best model weights: C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "Training epochs on cpu:  20%|██        | 4/20 [00:12<00:40,  2.53s/epoch, loss=0.31, prev_loss=0.327] WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 40.84s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 40.84s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 5: 0.006800529118475053. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 5.\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 5: 0.006800529118475053. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 5.\n",
      "Training epochs on cpu:  45%|████▌     | 9/20 [01:02<00:54,  4.91s/epoch, loss=0.277, prev_loss=0.283]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 39.74s seconds\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 39.74s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 10: 0.008818641250890344. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 10: 0.008818641250890344. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "Training epochs on cpu:  50%|█████     | 10/20 [01:42<02:42, 16.24s/epoch, loss=0.277, prev_loss=0.283]INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 10.\n",
      "Training epochs on cpu:  70%|███████   | 14/20 [01:51<00:31,  5.27s/epoch, loss=0.249, prev_loss=0.251]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 38.12s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 15: 0.010260149916901265. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 15.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 38.12s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 15: 0.010260149916901265. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 15.\n",
      "Training epochs on cpu:  95%|█████████▌| 19/20 [02:37<00:04,  4.96s/epoch, loss=0.218, prev_loss=0.226]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 37.27s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 20: 0.010378862395278635. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 20.\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 37.27s seconds\n",
      "INFO:pykeen.stoppers.early_stopping:New best result at epoch 20: 0.010378862395278635. Saved model weights to C:\\Users\\maxmo\\.data\\pykeen\\checkpoints\\best-model-weights-fe47dbae-9e4d-4a69-b130-a0aa1887f96d.pt\n",
      "INFO:pykeen.training.training_loop:=> Saved checkpoint after having finished epoch 20.\n",
      "Training epochs on cpu: 100%|██████████| 20/20 [03:14<00:00,  9.75s/epoch, loss=0.218, prev_loss=0.226]\n",
      "\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/29.0k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 29.0k/29.0k [00:39<00:00, 744triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 39.77s seconds\n",
      "Evaluating on cpu: 100%|██████████| 29.0k/29.0k [00:39<00:00, 744triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 39.77s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 29min 25s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch.manual_seed(seed=0)\n",
    "np.random.seed(seed=0)\n",
    "transe_train = pipeline(\n",
    "    model=TransE,\n",
    "    model_kwargs={\"random_seed\": 0},\n",
    "    training=training_set,\n",
    "    dimensions=64,\n",
    "    epochs=20,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    # early stopping based on the validation set during training\n",
    "    stopper=\"early\",\n",
    "    stopper_kwargs={\n",
    "        \"frequency\": 5,  # frequency: number of epochs after which an evaluation is made by the stopper\n",
    "        \"patience\": 10,  # patience: number of consecutive runs with no improvement, wait until which to stop\n",
    "        \"relative_delta\": 0.0,\n",
    "    },  # relative delta: the minimum increase/decrease in metric to be considered as improvement\n",
    "    validation=validation_set,\n",
    "    # evaluate model performance on the test set\n",
    "    testing=testing_set,\n",
    "    evaluator=RankBasedEvaluator(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4ba6c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test['edge_type'] = ratings_test['rating'].apply(\n",
    "    lambda x: 'liked' if x >= 4.0 else 'disliked' if x < 2.0 else 'neutral'\n",
    ")\n",
    "\n",
    "# Drop rows with 'neutral' edge_type\n",
    "#ratings_test = ratings_test[ratings_test['edge_type'] != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4c427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ccf66906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_edge_type(model, user2id, movie2id, ratings_test):\n",
    "    predictions = []\n",
    "    \n",
    "    # Get ID mappings\n",
    "    entity_to_id = model.training.entity_to_id\n",
    "    relation_to_id = model.training.relation_to_id\n",
    "\n",
    "    for _, row in ratings_test.iterrows():\n",
    "        user = user2id.get(row['userId'])\n",
    "        movie = movie2id.get(row['movieId'])\n",
    "        if user is None or movie is None:\n",
    "            predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Map to numeric IDs\n",
    "            h_id = entity_to_id[str(user)]\n",
    "            t_id = entity_to_id[str(movie)]\n",
    "            liked_id = relation_to_id['liked']\n",
    "            disliked_id = relation_to_id['disliked']\n",
    "\n",
    "            # Score the triples\n",
    "            liked_score = model.model.score_hrt(torch.tensor([[h_id, liked_id, t_id]])).item()\n",
    "            disliked_score = model.model.score_hrt(torch.tensor([[h_id, disliked_id, t_id]])).item()\n",
    "\n",
    "            predicted_edge_type = 'liked' if liked_score > disliked_score else 'disliked'\n",
    "            predictions.append(predicted_edge_type)\n",
    "        except KeyError:\n",
    "            predictions.append(None)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e9760b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_edges(model, user_tensor, movie_tensor, ratings_test):\n",
    "    \"\"\"\n",
    "    Scores edges for a batch of user-movie pairs.\n",
    "    Args:\n",
    "        model: Trained PyKEEN model.\n",
    "        user_tensor: Tensor of user IDs.\n",
    "        movie_tensor: Tensor of movie IDs.\n",
    "        ratings_test: DataFrame containing test ratings.\n",
    "    Returns:\n",
    "        Tensor of scores for the given edges.\n",
    "    \"\"\"\n",
    "    # Get ID mappings\n",
    "    entity_to_id = model.training.entity_to_id\n",
    "    relation_to_id = model.training.relation_to_id\n",
    "\n",
    "    # Map to numeric IDs\n",
    "    h_ids = user_tensor.cpu().numpy()\n",
    "    t_ids = movie_tensor.cpu().numpy()\n",
    "    liked_id = relation_to_id.get('liked')\n",
    "\n",
    "    # Validate IDs\n",
    "    valid_h_ids = [h_id for h_id in h_ids if str(h_id) in entity_to_id]\n",
    "    valid_t_ids = [t_id for t_id in t_ids if str(t_id) in entity_to_id]\n",
    "    if not valid_h_ids or not valid_t_ids or liked_id is None:\n",
    "        raise ValueError(\"Invalid entity or relation IDs detected.\")\n",
    "\n",
    "    # Score the triples\n",
    "    scores = model.model.score_hrt(\n",
    "        torch.tensor([[h_id, liked_id, t_id] for h_id, t_id in zip(valid_h_ids, valid_t_ids)], device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "af13188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_test['predicted_edge_type_transe'] = predict_edge_type(transe_train, user2id, movie2id, ratings_test)\n",
    "ratings_test['predicted_edge_type_transh'] = predict_edge_type(transH_train, user2id, movie2id, ratings_test)\n",
    "ratings_test['predicted_edge_type_RESCAL'] = predict_edge_type(rescal_train, user2id, movie2id, ratings_test)\n",
    "# Add predictions for DistMult and RESCAL if trained models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "225b0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare variables for build_eval_df and evaluate_recommendations\n",
    "user_movie_test_edges_list = list(zip(\n",
    "    ratings_test['userId'].map(user2id),\n",
    "    ratings_test['movieId'].map(movie2id)\n",
    "))\n",
    "\n",
    "user_movie_edges_list = list(zip(\n",
    "    ratings_train['userId'].map(user2id),\n",
    "    ratings_train['movieId'].map(movie2id)\n",
    "))\n",
    "\n",
    "user_movie_val_edges_list = list(zip(\n",
    "    ratings_val['userId'].map(user2id),\n",
    "    ratings_val['movieId'].map(movie2id)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "39ac86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movie_idx = pd.Series(movie_ids).map(movie2id).to_numpy()\n",
    "\n",
    "def build_eval_df(model,\n",
    "    test_pos_edges=user_movie_test_edges_list,\n",
    "    user_movie_edges_list=user_movie_edges_list,\n",
    "    user_movie_val_edges_list=user_movie_val_edges_list,\n",
    "    all_movie_ids=all_movie_idx):\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    # Group the test positives by user\n",
    "    from collections import defaultdict\n",
    "    pos_by_user = defaultdict(list)\n",
    "    for u, m in test_pos_edges:\n",
    "        pos_by_user[u].append(m)\n",
    "    \n",
    "    for user_id, pos_movies in pos_by_user.items():\n",
    "        # Precompute forbidden set for this user\n",
    "        forbidden = set(pos_movies) \\\n",
    "                    | {m for (u, m) in user_movie_edges_list    if u == user_id} \\\n",
    "                    | {m for (u, m) in user_movie_val_edges_list if u == user_id}\n",
    "        \n",
    "        for pos_m in pos_movies:\n",
    "            # 1 positive\n",
    "            batch_movies = [pos_m]\n",
    "            labels       = [1]\n",
    "            \n",
    "            # 9 negatives sampled uniformly\n",
    "            neg_samples = []\n",
    "            while len(neg_samples) < 9:\n",
    "                m = all_movie_ids[torch.randint(len(all_movie_ids), (1,)).item()]\n",
    "                if m not in forbidden:\n",
    "                    neg_samples.append(m)\n",
    "                    forbidden.add(m)  # avoid duplicates\n",
    "            batch_movies += neg_samples\n",
    "            labels       += [0] * len(neg_samples)\n",
    "            \n",
    "            # Score the batch of 10 edges\n",
    "            u_tensor  = torch.tensor([user_id] * len(batch_movies), device=device)\n",
    "            m_tensor  = torch.tensor(batch_movies,      device=device)\n",
    "            with torch.no_grad():\n",
    "                scores = score_edges(model, u_tensor, m_tensor, ratings_test_filtered).cpu().numpy()\n",
    "            \n",
    "            # Accumulate rows\n",
    "            for m, s, gt in zip(batch_movies, scores, labels):\n",
    "                rows.append({\n",
    "                    'user': user_id,\n",
    "                    'movie': m,\n",
    "                    'edge_score': float(s),\n",
    "                    'ground_truth': gt\n",
    "                })\n",
    "    \n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(rows, columns=['user','movie','edge_score','ground_truth'])\n",
    "    return df\n",
    "\n",
    "def evaluate_recommendations(df, ks):\n",
    "    \"\"\"\n",
    "    Expects args:\n",
    "        df: pandas df with columns ['user', 'movie', 'edge_score', 'ground_truth']\n",
    "        ks (list of ints): Numbers of top recommendations to consider\n",
    "\n",
    "    Returns NDCG@k, Recall@k, MRR@k for the given k\n",
    "    \"\"\"\n",
    "    # Group by user\n",
    "    users = df['user'].unique()\n",
    "    for k in ks:\n",
    "        ndcg_list = []\n",
    "        recall_list = []\n",
    "        rr_list = []\n",
    "    \n",
    "        for user in users:\n",
    "            user_df = df[df['user'] == user]\n",
    "            # Sort by predicted score\n",
    "            ranked = user_df.sort_values('edge_score', ascending=False)\n",
    "            # Top k predictions\n",
    "            topk = ranked.head(k)\n",
    "            # Ground truth relevance values\n",
    "            rel = topk['ground_truth'].values\n",
    "            # Compute DCG@k\n",
    "            gains = (2**rel - 1)\n",
    "            discounts = np.log2(np.arange(2, k + 2))\n",
    "            dcg = np.sum(gains / discounts)\n",
    "    \n",
    "            # Compute IDCG@k (ideal ranking)\n",
    "            ideal_rel = np.sort(user_df['ground_truth'].values)[::-1][:k]\n",
    "            ideal_gains = (2**ideal_rel - 1)\n",
    "            idcg = np.sum(ideal_gains / discounts)\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "            ndcg_list.append(ndcg)\n",
    "    \n",
    "            # Recall@k: number relevant in topk / total relevant\n",
    "            total_rel = user_df['ground_truth'].sum()\n",
    "            recall = rel.sum() / total_rel if total_rel > 0 else 0.0\n",
    "            recall_list.append(recall)\n",
    "    \n",
    "            # MRR@k: reciprocal of rank of the top-ranked true edge\n",
    "            rr = 0.0\n",
    "            for idx, val in enumerate(rel, start=1):\n",
    "                if val == 1:\n",
    "                    rr = 1.0 / idx\n",
    "                    break\n",
    "            rr_list.append(rr)\n",
    "    \n",
    "        dict_of_results = {\n",
    "            f'NDCG@{k}': np.mean(ndcg_list),\n",
    "            f'Recall@{k}': np.mean(recall_list),\n",
    "            f'MRR@{k}': np.mean(rr_list)\n",
    "        }\n",
    "        # Aggregate metrics\n",
    "        print(f'METRICS FOR {k}:\\n{dict_of_results}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f1ea37a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxmo\\AppData\\Local\\Temp\\ipykernel_2132\\2923235229.py:49: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  'edge_score': float(s),\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m [transe_train, transH_train, rescal_train]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Build evaluation DataFrame\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df_for_recs \u001b[38;5;241m=\u001b[39m build_eval_df(\n\u001b[0;32m      4\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      5\u001b[0m         test_pos_edges\u001b[38;5;241m=\u001b[39muser_movie_test_edges_list,\n\u001b[0;32m      6\u001b[0m         user_movie_edges_list\u001b[38;5;241m=\u001b[39muser_movie_edges_list,\n\u001b[0;32m      7\u001b[0m         user_movie_val_edges_list\u001b[38;5;241m=\u001b[39muser_movie_val_edges_list,\n\u001b[0;32m      8\u001b[0m         all_movie_ids\u001b[38;5;241m=\u001b[39mall_movie_idx)\n\u001b[0;32m      9\u001b[0m     evaluate_recommendations(df\u001b[38;5;241m=\u001b[39mdf_for_recs, ks\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m])\n",
      "Cell \u001b[1;32mIn[120], line 42\u001b[0m, in \u001b[0;36mbuild_eval_df\u001b[1;34m(model, test_pos_edges, user_movie_edges_list, user_movie_val_edges_list, all_movie_ids)\u001b[0m\n\u001b[0;32m     40\u001b[0m m_tensor  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_movies,      device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 42\u001b[0m     scores \u001b[38;5;241m=\u001b[39m score_edges(model, u_tensor, m_tensor, ratings_test_filtered)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Accumulate rows\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m, s, gt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_movies, scores, labels):\n",
      "Cell \u001b[1;32mIn[119], line 28\u001b[0m, in \u001b[0;36mscore_edges\u001b[1;34m(model, user_tensor, movie_tensor, ratings_test)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid entity or relation IDs detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Score the triples\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mscore_hrt(\n\u001b[0;32m     29\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor([[h_id, liked_id, t_id] \u001b[38;5;28;01mfor\u001b[39;00m h_id, t_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(valid_h_ids, valid_t_ids)], device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\pykeen\\models\\nbase.py:495\u001b[0m, in \u001b[0;36mERModel.score_hrt\u001b[1;34m(self, hrt_batch, mode)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03mThis method takes head, relation and tail of each triple and calculates the corresponding score.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    The score for each triple.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# Note: slicing cannot be used here: the indices for score_hrt only have a batch\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# dimension, and slicing along this dimension is already considered by sub-batching.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Note: we do not delegate to the general method for performance reasons\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# Note: repetition is not necessary here\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m h, r, t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_representations(h\u001b[38;5;241m=\u001b[39mhrt_batch[:, \u001b[38;5;241m0\u001b[39m], r\u001b[38;5;241m=\u001b[39mhrt_batch[:, \u001b[38;5;241m1\u001b[39m], t\u001b[38;5;241m=\u001b[39mhrt_batch[:, \u001b[38;5;241m2\u001b[39m], mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteraction\u001b[38;5;241m.\u001b[39mscore_hrt(h\u001b[38;5;241m=\u001b[39mh, r\u001b[38;5;241m=\u001b[39mr, t\u001b[38;5;241m=\u001b[39mt)\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\pykeen\\models\\nbase.py:671\u001b[0m, in \u001b[0;36mERModel._get_representations\u001b[1;34m(self, h, r, t, mode)\u001b[0m\n\u001b[0;32m    669\u001b[0m head_representations \u001b[38;5;241m=\u001b[39m [head_representations[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteraction\u001b[38;5;241m.\u001b[39mhead_indices]\n\u001b[0;32m    670\u001b[0m tail_representations \u001b[38;5;241m=\u001b[39m [tail_representations[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteraction\u001b[38;5;241m.\u001b[39mtail_indices]\n\u001b[1;32m--> 671\u001b[0m hr, rr, tr \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    672\u001b[0m     [representation(indices\u001b[38;5;241m=\u001b[39mindices) \u001b[38;5;28;01mfor\u001b[39;00m representation \u001b[38;5;129;01min\u001b[39;00m representations]\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m indices, representations \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    674\u001b[0m         (h, head_representations),\n\u001b[0;32m    675\u001b[0m         (r, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelation_representations),\n\u001b[0;32m    676\u001b[0m         (t, tail_representations),\n\u001b[0;32m    677\u001b[0m     )\n\u001b[0;32m    678\u001b[0m )\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m# normalization\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28mtuple\u001b[39m[HeadRepresentation, RelationRepresentation, TailRepresentation],\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (hr, rr, tr)),\n\u001b[0;32m    683\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\pykeen\\models\\nbase.py:672\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    669\u001b[0m head_representations \u001b[38;5;241m=\u001b[39m [head_representations[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteraction\u001b[38;5;241m.\u001b[39mhead_indices]\n\u001b[0;32m    670\u001b[0m tail_representations \u001b[38;5;241m=\u001b[39m [tail_representations[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteraction\u001b[38;5;241m.\u001b[39mtail_indices]\n\u001b[0;32m    671\u001b[0m hr, rr, tr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 672\u001b[0m     [representation(indices\u001b[38;5;241m=\u001b[39mindices) \u001b[38;5;28;01mfor\u001b[39;00m representation \u001b[38;5;129;01min\u001b[39;00m representations]\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m indices, representations \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m    674\u001b[0m         (h, head_representations),\n\u001b[0;32m    675\u001b[0m         (r, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelation_representations),\n\u001b[0;32m    676\u001b[0m         (t, tail_representations),\n\u001b[0;32m    677\u001b[0m     )\n\u001b[0;32m    678\u001b[0m )\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m# normalization\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28mtuple\u001b[39m[HeadRepresentation, RelationRepresentation, TailRepresentation],\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (hr, rr, tr)),\n\u001b[0;32m    683\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\pykeen\\nn\\representation.py:236\u001b[0m, in \u001b[0;36mRepresentation.forward\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique:\n\u001b[0;32m    235\u001b[0m     indices, inverse \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39munique(return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 236\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plain_forward(indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# normalize *before* repeating\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\pykeen\\nn\\representation.py:498\u001b[0m, in \u001b[0;36mEmbedding._plain_forward\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    497\u001b[0m     prefix_shape \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 498\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embeddings(indices\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m    499\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mprefix_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape)\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# fixme: work-around until nn.Embedding supports complex\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\maxmo\\anaconda3\\envs\\GRL_env\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "for model in [transe_train, transH_train, rescal_train]:\n",
    "    # Build evaluation DataFrame\n",
    "    df_for_recs = build_eval_df(\n",
    "        model=model,\n",
    "        test_pos_edges=user_movie_test_edges_list,\n",
    "        user_movie_edges_list=user_movie_edges_list,\n",
    "        user_movie_val_edges_list=user_movie_val_edges_list,\n",
    "        all_movie_ids=all_movie_idx)\n",
    "    evaluate_recommendations(df=df_for_recs, ks=[3,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390573eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage agreement for predicted_edge_type_transe: 63.18%\n",
      "Percentage agreement for predicted_edge_type_transh: 88.72%\n",
      "Percentage agreement for predicted_edge_type_RESCAL: 46.65%\n"
     ]
    }
   ],
   "source": [
    "for column in ['predicted_edge_type_transe', 'predicted_edge_type_transh', 'predicted_edge_type_RESCAL']:\n",
    "    agreement = (ratings_test[column] == ratings_test['edge_type']).mean() * 100\n",
    "    print(f\"Percentage agreement for {column}: {agreement:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
