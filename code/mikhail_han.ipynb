{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccf44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/volkovm/.local/lib/python3.11/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.27' not found (required by /zfs/users/volkovm/.local/lib/python3.11/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/users/volkovm/.local/lib/python3.11/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /zfs/users/volkovm/.local/lib/python3.11/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/users/volkovm/.local/lib/python3.11/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /zfs/users/volkovm/.local/lib/python3.11/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/users/volkovm/.local/lib/python3.11/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /zfs/users/volkovm/.local/lib/python3.11/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(\n",
      "/users/volkovm/.local/lib/python3.11/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /zfs/users/volkovm/.local/lib/python3.11/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ast\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HANConv\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e496829-513d-4a13-9cae-52c78d4969d4",
   "metadata": {},
   "source": [
    "# Generate PyG graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5438b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_scores = pd.read_csv('full_data/genome_scores_processed.csv')\n",
    "movies = pd.read_csv('full_data/movies_warm.csv')\n",
    "ratings_train = pd.read_csv('full_data/ratings_train.csv')\n",
    "ratings_test = pd.read_csv('full_data/ratings_test.csv')\n",
    "ratings_val = pd.read_csv('full_data/ratings_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c584d90-3a6d-4213-83d6-c06fe103b29d",
   "metadata": {},
   "source": [
    "Extract users, movies and tags that are going to serve as our nodes. We need all of the users (i.e., from train, test and val) because HAN needs to generate embeddings for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0b600d-9c35-44ce-b186-a13fb09d9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = np.unique(np.concatenate([\n",
    "    ratings_train['userId'].unique(),\n",
    "    ratings_val['userId'].unique(),\n",
    "    ratings_test['userId'].unique()\n",
    "]))\n",
    "movie_ids = movies['movieId'].unique()\n",
    "unique_genres = {'Action',\n",
    " 'Adventure',\n",
    " 'Animation',\n",
    " 'Children',\n",
    " 'Comedy',\n",
    " 'Crime',\n",
    " 'Documentary',\n",
    " 'Drama',\n",
    " 'Fantasy',\n",
    " 'Film-Noir',\n",
    " 'Horror',\n",
    " 'IMAX',\n",
    " 'Musical',\n",
    " 'Mystery',\n",
    " 'Romance',\n",
    " 'Sci-Fi',\n",
    " 'Thriller',\n",
    " 'War',\n",
    " 'Western'}\n",
    "tags = genome_scores['tagId'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d8340",
   "metadata": {},
   "source": [
    "Now, add all of the edges and their inverses (inverses are needed for metapaths to be intelligible). Recall that in original form, HAN does not handle weighted edges. So, we need a way to turn genome score relation into a binary relation (`has tag / does not have tag'). We therefore need some relevance-threshold for cutting off when a movie has a tag vs when it doesn't. Due to the binary treatment, I imagine the threshold has to be fairly high - especially since the genome scoring seems to associate movies with tags very tightly (every movie has at least 1 tag with >0.99 relevance)\n",
    "\n",
    "\n",
    "Same for the ratings. There will be an edge if rating is `positive' in the csv (>= 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ddbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_threshold = 0.5\n",
    "genome_scores_filtered = genome_scores[genome_scores['relevance'] > relevance_threshold]\n",
    "genome_scores_filtered = genome_scores_filtered[genome_scores_filtered['movieId'].isin(movie_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9059c-4515-4a06-a073-177573532537",
   "metadata": {},
   "source": [
    "For the PyG graph, we will need to assign indeces to nodes in each node type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8485fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie2id = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "N_nodes = len(movie_ids)\n",
    "user2id = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "N_nodes += len(user_ids)\n",
    "tag2id = {tag_id: idx for idx, tag_id in enumerate(tags)}\n",
    "N_nodes += len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93291a06-781e-4e28-b171-974d69ecf52b",
   "metadata": {},
   "source": [
    "Add relevant edges and their inverses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e2a4aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16032/2879656289.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_train_filtered['user_idx'] = ratings_train_filtered['userId'].map(user2id)\n",
      "/tmp/ipykernel_16032/2879656289.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_train_filtered['movie_idx'] = ratings_train_filtered['movieId'].map(movie2id)\n"
     ]
    }
   ],
   "source": [
    "# Movie-Tag (and Tag-Movie)\n",
    "genome_scores_filtered['movie_idx'] = genome_scores_filtered['movieId'].map(movie2id)\n",
    "genome_scores_filtered['tag_idx'] = genome_scores_filtered['tagId'].map(tag2id)\n",
    "\n",
    "movie_tag_edges_list = list(zip(genome_scores_filtered['movie_idx'], genome_scores_filtered['tag_idx']))\n",
    "movie_tag_inv_edges_list = list(zip(genome_scores_filtered['tag_idx'], genome_scores_filtered['movie_idx']))\n",
    "\n",
    "# User-Movie (and Movie-User)\n",
    "ratings_train_filtered = ratings_train[ratings_train['positive_rating'] == 1]\n",
    "\n",
    "ratings_train_filtered['user_idx'] = ratings_train_filtered['userId'].map(user2id)\n",
    "ratings_train_filtered['movie_idx'] = ratings_train_filtered['movieId'].map(movie2id)\n",
    "\n",
    "user_movie_edges_list = list(zip(ratings_train_filtered['user_idx'], ratings_train_filtered['movie_idx']))\n",
    "\n",
    "user_movie_inv_edges_list = list(zip(ratings_train_filtered['movie_idx'], ratings_train_filtered['user_idx']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46306bd9-1e5a-4c3e-9988-aab22ae14469",
   "metadata": {},
   "source": [
    "Read our nodes and lists of edges into the PyG graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab5a2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data['user'].num_nodes = len(user_ids)\n",
    "data['movie'].num_nodes = len(movie_ids)\n",
    "# data['genre'].num_nodes = len(unique_genres)\n",
    "data['tag'].num_nodes = len(tags)\n",
    "\n",
    "# Movie—Tag-Movie\n",
    "src, dst = zip(*movie_tag_edges_list)  # lists of (movie_idx, genre_idx)\n",
    "data['movie', 'hasTag', 'tag'].edge_index = torch.tensor([src, dst])\n",
    "data['tag', 'inv_hasTag', 'movie'].edge_index = torch.tensor([dst, src])\n",
    "\n",
    "# User-Movie-User\n",
    "src, dst = zip(*user_movie_edges_list)\n",
    "data['user', 'Likes', 'movie'].edge_index = torch.tensor([src, dst])\n",
    "data['movie', 'inv_Likes', 'user'].edge_index = torch.tensor([dst, src])\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ed4e4-2632-4402-9c3a-af63eed657bb",
   "metadata": {},
   "source": [
    "# HAN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d4a6c-3641-4502-b2a7-edfe98c04cbe",
   "metadata": {},
   "source": [
    "Embedding dimension is 64, as in the paper. \n",
    "\n",
    "NOTE! : If you want to use features in movie node embeddings, skip the cell below, uncomment and run the cell after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff8c76f1-ad53-475c-b048-96f4ebbef958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_dim = 64  \n",
    "\n",
    "# user_emb  = nn.Embedding(data['user'].num_nodes,  hidden_dim)\n",
    "# movie_emb = nn.Embedding(data['movie'].num_nodes, hidden_dim)\n",
    "# tag_emb   = nn.Embedding(data['tag'].num_nodes,   hidden_dim)\n",
    "\n",
    "# metadata = data.metadata()  \n",
    "\n",
    "# conv = HANConv(\n",
    "#     in_channels  = {\n",
    "#       'user':  hidden_dim,\n",
    "#       'movie': hidden_dim,\n",
    "#       'tag':   hidden_dim,\n",
    "#     },\n",
    "#     out_channels = hidden_dim,\n",
    "#     metadata     = metadata,\n",
    "#     heads        = 4,       # number of attention heads\n",
    "#     dropout      = 0.5,     # attention‐head dropout\n",
    "# )\n",
    "\n",
    "# def forward_han(data):\n",
    "#     x_dict = {\n",
    "#         'user':  user_emb(torch.arange(data['user'].num_nodes,  device=device)),\n",
    "#         'movie': movie_emb(torch.arange(data['movie'].num_nodes, device=device)),\n",
    "#         'tag':   tag_emb(torch.arange(data['tag'].num_nodes,   device=device)),\n",
    "#     }\n",
    "\n",
    "#     x_dict = conv(x_dict, data.edge_index_dict)\n",
    "#     return x_dict\n",
    "\n",
    "# x_dict = forward_han(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c0493a-c11e-4567-9a15-04982cefeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_lists = movies['genre_list'].apply(lambda s: ast.literal_eval(s)).tolist()\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_features = mlb.fit_transform(genre_lists)\n",
    "movie_features_tensor = torch.tensor(genre_features, dtype=torch.float32, device=device)\n",
    "movie_features_tensor = movie_features_tensor.to(device)\n",
    "\n",
    "hidden_dim = 64  \n",
    "\n",
    "user_emb  = nn.Embedding(data['user'].num_nodes,  hidden_dim)\n",
    "movie_emb = nn.Embedding(data['movie'].num_nodes, hidden_dim)\n",
    "input_dim_movie = movie_features_tensor.shape[1] + hidden_dim\n",
    "tag_emb   = nn.Embedding(data['tag'].num_nodes,   hidden_dim)\n",
    "\n",
    "metadata = data.metadata()  \n",
    "\n",
    "conv = HANConv(\n",
    "    in_channels  = {\n",
    "      'user':  hidden_dim,\n",
    "      'movie': input_dim_movie,\n",
    "      'tag':   hidden_dim,\n",
    "    },\n",
    "    out_channels = hidden_dim,\n",
    "    metadata     = metadata,\n",
    "    heads        = 4,       # number of attention heads\n",
    "    dropout      = 0.5,     # attention‐head dropout\n",
    ")\n",
    "\n",
    "def forward_han(data):\n",
    "    learned_movie_emb = movie_emb(torch.arange(data['movie'].num_nodes, device=device))\n",
    "    movie_x = torch.cat([movie_features_tensor, learned_movie_emb], dim=1)\n",
    "    \n",
    "    x_dict = {\n",
    "        'user':  user_emb(torch.arange(data['user'].num_nodes,  device=device)),\n",
    "        'movie': movie_x,\n",
    "        'tag':   tag_emb(torch.arange(data['tag'].num_nodes, device=device)),\n",
    "    }\n",
    "\n",
    "    x_dict = conv(x_dict, data.edge_index_dict)\n",
    "    return x_dict\n",
    "\n",
    "x_dict = forward_han(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fa8ba-63ea-4c25-bbcd-e16ee83b7394",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a538860-84ee-43b5-a138-38dbc1024015",
   "metadata": {},
   "source": [
    "I use 30 as negative sample size in train, validation and test. 5 is used in the paper - but they consider link prediction, not recommendation. It seems that the model has to be more discriminatory in the recommendation task, so there is reason for higher negative sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db58654a-2a91-4cf6-a32d-5334dab8312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "validation_frequency = 5\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "num_negatives = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6ffd1-ac15-46c7-8680-27eeaa3da165",
   "metadata": {},
   "source": [
    "We will employ early stopping in our training. Once every several epochs, we will compute validation loss of the current model. Let us define the function that will compute this loss in advance.\n",
    "\n",
    "We can only use those users and movies from validation that HAN has seen from train because those are the only ones we have embeddings for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2a7df10-1e56-45de-b4fb-47cae137849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16032/2840290524.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_val_filtered['user_idx'] = ratings_val_filtered['userId'].map(user2id)\n",
      "/tmp/ipykernel_16032/2840290524.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_val_filtered['movie_idx'] = ratings_val_filtered['movieId'].map(movie2id)\n"
     ]
    }
   ],
   "source": [
    "seen_users = set(ratings_train_filtered['userId'].unique())\n",
    "seen_movies = set(ratings_train_filtered['movieId'].unique())\n",
    "\n",
    "# filter validation data to only include seen users\n",
    "ratings_val_filtered = ratings_val[\n",
    "    (ratings_val['positive_rating'] == 1) &\n",
    "    (ratings_val['userId'].isin(seen_users)) &\n",
    "    (ratings_val['movieId'].isin(seen_movies))\n",
    "]\n",
    "\n",
    "# Map to indices\n",
    "ratings_val_filtered['user_idx'] = ratings_val_filtered['userId'].map(user2id)\n",
    "ratings_val_filtered['movie_idx'] = ratings_val_filtered['movieId'].map(movie2id)\n",
    "\n",
    "# Create edge list\n",
    "user_movie_val_edges_list = list(zip(ratings_val_filtered['user_idx'], ratings_val_filtered['movie_idx']))\n",
    "\n",
    "val_user, val_movie = zip(*user_movie_val_edges_list)\n",
    "\n",
    "val_user = torch.tensor(val_user, device=device)\n",
    "val_movie = torch.tensor(val_movie, device=device)\n",
    "\n",
    "def compute_validation_loss(data, val_user, val_movie, num_neg_samples=1):\n",
    "    conv.eval()  # evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get current node embeddings from the model\n",
    "        x_dict = forward_han(data)\n",
    "\n",
    "        # Positive scores (actual interactions)\n",
    "        pos_scores = score_edges(val_user, val_movie, x_dict)\n",
    "\n",
    "        # Sample negative movies (not interacted with by user)\n",
    "        neg_user = val_user.repeat(num_negatives, 1).flatten()\n",
    "        neg_movie = torch.randint(0, data['movie'].num_nodes, (len(neg_user),), device=device)\n",
    "\n",
    "        neg_scores = score_edges(neg_user, neg_movie, x_dict)\n",
    "\n",
    "        # Loss = average binary cross-entropy loss\n",
    "        loss = -torch.log(pos_scores + 1e-15).mean() - torch.log(1 - neg_scores + 1e-15).mean()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b077235-adc8-41ce-92b9-2a76b4441d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5\n",
      "epoch 10\n",
      "epoch 15\n",
      "epoch 20\n",
      "epoch 25\n",
      "epoch 30\n",
      "epoch 35\n",
      "epoch 40\n",
      "epoch 45\n",
      "epoch 50\n",
      "epoch 55\n",
      "epoch 60\n",
      "epoch 65\n",
      "epoch 70\n",
      "epoch 75\n",
      "epoch 80\n",
      "epoch 85\n",
      "epoch 90\n",
      "epoch 95\n",
      "epoch 100\n",
      "epoch 105\n",
      "epoch 110\n",
      "epoch 115\n",
      "epoch 120\n",
      "epoch 125\n",
      "epoch 130\n",
      "epoch 135\n",
      "epoch 140\n",
      "epoch 145\n",
      "epoch 150\n",
      "epoch 155\n",
      "epoch 160\n",
      "epoch 165\n",
      "epoch 170\n",
      "epoch 175\n",
      "epoch 180\n",
      "epoch 185\n",
      "epoch 190\n",
      "epoch 195\n",
      "epoch 200\n",
      "epoch 205\n",
      "epoch 210\n",
      "epoch 215\n",
      "epoch 220\n",
      "epoch 225\n",
      "epoch 230\n",
      "epoch 235\n",
      "epoch 240\n",
      "epoch 245\n",
      "Early stopping at epoch 245\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "optimizer = torch.optim.Adam(list(conv.parameters()) +\n",
    "                             list(user_emb.parameters()) +\n",
    "                             list(movie_emb.parameters()), lr=0.01)\n",
    "\n",
    "def score_edges(u_idx, m_idx, x_dict):\n",
    "    hu = x_dict['user'][u_idx.long()]\n",
    "    hm = x_dict['movie'][m_idx.long()]\n",
    "    return torch.sigmoid((hu * hm).sum(dim=-1))\n",
    "    \n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    conv.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Positive samples\n",
    "    pos_user, pos_movie = zip(*user_movie_edges_list)  \n",
    "    pos_user = torch.tensor(pos_user, device=device)\n",
    "    pos_movie = torch.tensor(pos_movie, device=device)\n",
    "\n",
    "    # Negative sampling (random movies for each user)\n",
    "    neg_user = pos_user.repeat(num_negatives, 1).flatten()  \n",
    "    neg_movie = torch.randint(0, data['movie'].num_nodes, (num_negatives * len(pos_user),), device=device)\n",
    "\n",
    "    x_dict = forward_han(data)\n",
    "    pos_scores = score_edges(pos_user, pos_movie, x_dict)\n",
    "    neg_scores = score_edges(neg_user, neg_movie, x_dict)\n",
    "\n",
    "    loss = -torch.log(pos_scores + 1e-15).mean() - torch.log(1 - neg_scores + 1e-15).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "       # validate\n",
    "    if epoch % validation_frequency == 0:\n",
    "        print(f'epoch {epoch}')\n",
    "        val_loss = compute_validation_loss(data, val_user, val_movie)  # This function computes validation loss\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(conv.state_dict(), 'best_model.pt')  # Save best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005aa662-5c52-405e-8a55-e0e8d122cd2a",
   "metadata": {},
   "source": [
    "# TEST\n",
    "\n",
    "The logic with the test set is the same as with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25c746f5-25ee-450c-b763-6f3ffd09dd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16032/2170636102.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_test_filtered['user_idx'] = ratings_test_filtered['userId'].map(user2id)\n",
      "/tmp/ipykernel_16032/2170636102.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_test_filtered['movie_idx'] = ratings_test_filtered['movieId'].map(movie2id)\n"
     ]
    }
   ],
   "source": [
    "# filter test data to only include seen users\n",
    "ratings_test_filtered = ratings_test[\n",
    "    (ratings_test['positive_rating'] == 1) &\n",
    "    (ratings_test['userId'].isin(seen_users)) &\n",
    "    (ratings_test['movieId'].isin(seen_movies))\n",
    "]\n",
    "\n",
    "# Map to indices\n",
    "ratings_test_filtered['user_idx'] = ratings_test_filtered['userId'].map(user2id)\n",
    "ratings_test_filtered['movie_idx'] = ratings_test_filtered['movieId'].map(movie2id)\n",
    "\n",
    "# Create edge list\n",
    "user_movie_test_edges_list = list(zip(ratings_test_filtered['user_idx'], ratings_test_filtered['movie_idx']))\n",
    "\n",
    "test_user, test_movie = zip(*user_movie_test_edges_list)\n",
    "\n",
    "test_user = torch.tensor(test_user, device=device)\n",
    "test_movie = torch.tensor(test_movie, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9556a9-d544-4337-9ded-47636b2fcc16",
   "metadata": {},
   "source": [
    "Create evaluation function;\n",
    "assume input: df user | movie | score | true label (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c26e3c5b-cd71-4383-a4ef-da97e803f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_ranking_tensor(data, test_user, test_movie, k=3, num_negatives=num_negatives):\n",
    "#     conv.eval()\n",
    "#     with torch.no_grad():\n",
    "#         x_dict = forward_han(data)  # Get node embeddings\n",
    "\n",
    "#     hits, mrr = [], []\n",
    "\n",
    "#     for u, pos_m in zip(test_user, test_movie):\n",
    "#         u = u.item()\n",
    "#         pos_m = pos_m.item()\n",
    "\n",
    "#         # Skip if user or movie out of bounds\n",
    "#         if u >= data['user'].num_nodes or pos_m >= data['movie'].num_nodes:\n",
    "#             continue\n",
    "\n",
    "#         # Sample negative movies\n",
    "#         neg_movies = set()\n",
    "#         while len(neg_movies) < num_negatives:\n",
    "#             neg = random.randint(0, data['movie'].num_nodes - 1)\n",
    "#             if neg != pos_m:\n",
    "#                 neg_movies.add(neg)\n",
    "\n",
    "#         all_movies = [pos_m] + list(neg_movies)\n",
    "#         all_movies_tensor = torch.tensor(all_movies, device=device)\n",
    "#         user_tensor = torch.tensor([u] * len(all_movies), device=device)\n",
    "\n",
    "#         # Get scores\n",
    "#         scores = score_edges(user_tensor, all_movies_tensor, x_dict).cpu()\n",
    "#         _, ranking = scores.sort(descending=True)\n",
    "\n",
    "#         rank = (ranking == 0).nonzero(as_tuple=False).item()  # position of the positive sample\n",
    "\n",
    "#         hits.append(1 if rank < k else 0)\n",
    "#         mrr.append(1.0 / (rank + 1))\n",
    "\n",
    "#     hits_at_k = sum(hits) / len(hits) if hits else 0.0\n",
    "#     mean_mrr = sum(mrr) / len(mrr) if mrr else 0.0\n",
    "#     return hits_at_k, mean_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c69ead50-bad4-4dc6-9008-d32cd0ddb485",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movie_idx = pd.Series(movie_ids).map(movie2id).to_numpy()\n",
    "\n",
    "def build_eval_df(\n",
    "    test_pos_edges=user_movie_test_edges_list,\n",
    "    user_movie_edges_list=user_movie_edges_list,\n",
    "    user_movie_val_edges_list=user_movie_val_edges_list,\n",
    "    all_movie_ids=all_movie_idx):\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    # Group the test positives by user\n",
    "    from collections import defaultdict\n",
    "    pos_by_user = defaultdict(list)\n",
    "    for u, m in test_pos_edges:\n",
    "        pos_by_user[u].append(m)\n",
    "    \n",
    "    for user_id, pos_movies in pos_by_user.items():\n",
    "        # Precompute forbidden set for this user\n",
    "        forbidden = set(pos_movies) \\\n",
    "                    | {m for (u, m) in user_movie_edges_list    if u == user_id} \\\n",
    "                    | {m for (u, m) in user_movie_val_edges_list if u == user_id}\n",
    "        \n",
    "        for pos_m in pos_movies:\n",
    "            # 1 positive\n",
    "            batch_movies = [pos_m]\n",
    "            labels       = [1]\n",
    "            \n",
    "            # 9 negatives sampled uniformly\n",
    "            neg_samples = []\n",
    "            while len(neg_samples) < 9:\n",
    "                m = all_movie_ids[torch.randint(len(all_movie_ids), (1,)).item()]\n",
    "                if m not in forbidden:\n",
    "                    neg_samples.append(m)\n",
    "                    forbidden.add(m)  # avoid duplicates\n",
    "            batch_movies += neg_samples\n",
    "            labels       += [0] * len(neg_samples)\n",
    "            \n",
    "            # Score the batch of 10 edges\n",
    "            u_tensor  = torch.tensor([user_id] * len(batch_movies), device=device)\n",
    "            m_tensor  = torch.tensor(batch_movies,      device=device)\n",
    "            with torch.no_grad():\n",
    "                scores = score_edges(u_tensor, m_tensor, x_dict).cpu().numpy()\n",
    "            \n",
    "            # Accumulate rows\n",
    "            for m, s, gt in zip(batch_movies, scores, labels):\n",
    "                rows.append({\n",
    "                    'user': user_id,\n",
    "                    'movie': m,\n",
    "                    'edge_score': float(s),\n",
    "                    'ground_truth': gt\n",
    "                })\n",
    "    \n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(rows, columns=['user','movie','edge_score','ground_truth'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "788ff23b-bf45-4fab-a179-b7be7fce3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_recs = build_eval_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46810d7b-90b0-438b-a27f-7e5bcc051e7b",
   "metadata": {},
   "source": [
    "We will use 3 scorings we mentioned in the discussion and 9 negative sampled edges for each positive edge in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85d9d180-3173-4d1c-9541-3b9c5cfff16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(df, ks):\n",
    "    \"\"\"\n",
    "    Expects args:\n",
    "        df: pandas df with columns ['user', 'movie', 'edge_score', 'ground_truth']\n",
    "        ks (list of ints): Numbers of top recommendations to consider\n",
    "\n",
    "    Returns NDCG@k, Recall@k, MRR@k for the given k\n",
    "    \"\"\"\n",
    "    # Group by user\n",
    "    users = df['user'].unique()\n",
    "    for k in ks:\n",
    "        ndcg_list = []\n",
    "        recall_list = []\n",
    "        rr_list = []\n",
    "    \n",
    "        for user in users:\n",
    "            user_df = df[df['user'] == user]\n",
    "            # Sort by predicted score\n",
    "            ranked = user_df.sort_values('edge_score', ascending=False)\n",
    "            # Top k predictions\n",
    "            topk = ranked.head(k)\n",
    "            # Ground truth relevance values\n",
    "            rel = topk['ground_truth'].values\n",
    "            # Compute DCG@k\n",
    "            gains = (2**rel - 1)\n",
    "            discounts = np.log2(np.arange(2, k + 2))\n",
    "            dcg = np.sum(gains / discounts)\n",
    "    \n",
    "            # Compute IDCG@k (ideal ranking)\n",
    "            ideal_rel = np.sort(user_df['ground_truth'].values)[::-1][:k]\n",
    "            ideal_gains = (2**ideal_rel - 1)\n",
    "            idcg = np.sum(ideal_gains / discounts)\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "            ndcg_list.append(ndcg)\n",
    "    \n",
    "            # Recall@k: number relevant in topk / total relevant\n",
    "            total_rel = user_df['ground_truth'].sum()\n",
    "            recall = rel.sum() / total_rel if total_rel > 0 else 0.0\n",
    "            recall_list.append(recall)\n",
    "    \n",
    "            # MRR@k: reciprocal of rank of the top-ranked true edge\n",
    "            rr = 0.0\n",
    "            for idx, val in enumerate(rel, start=1):\n",
    "                if val == 1:\n",
    "                    rr = 1.0 / idx\n",
    "                    break\n",
    "            rr_list.append(rr)\n",
    "    \n",
    "        dict_of_results = {\n",
    "            f'NDCG@{k}': np.mean(ndcg_list),\n",
    "            f'Recall@{k}': np.mean(recall_list),\n",
    "            f'MRR@{k}': np.mean(rr_list)\n",
    "        }\n",
    "        # Aggregate metrics\n",
    "        print(f'METRICS FOR {k}:\\n{dict_of_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ae7d6-ca23-4c1f-8636-c81777b620b8",
   "metadata": {},
   "source": [
    "WITH EMBEDDINGS USING GENRES AS FEATURES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b397465-c1a9-4a62-bf15-f9c18db0bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS FOR 3:\n",
      "{'NDCG@3': 0.6188359648726435, 'Recall@3': 0.24589585919053558, 'MRR@3': 0.7460622678013983}\n",
      "METRICS FOR 5:\n",
      "{'NDCG@5': 0.6134050517976642, 'Recall@5': 0.37008681073168465, 'MRR@5': 0.7596879488183835}\n",
      "METRICS FOR 10:\n",
      "{'NDCG@10': 0.6301885242251088, 'Recall@10': 0.5555697502992681, 'MRR@10': 0.7646190233146754}\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations(df=df_for_recs, ks=[3,5,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edafb93-99b6-4e3e-aeda-f687547d21f0",
   "metadata": {},
   "source": [
    "WITH EMBEDDINGS *NOT* USING GENRES AS FEATURES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10c9d7c9-b126-410f-80be-c4adbbd47313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS FOR 3:\n",
      "{'NDCG@3': 0.5755917869833315, 'Recall@3': 0.2290656745774257, 'MRR@3': 0.7119372336763641}\n",
      "METRICS FOR 5:\n",
      "{'NDCG@5': 0.5711759024683697, 'Recall@5': 0.3467601615068751, 'MRR@5': 0.7276317819796082}\n",
      "METRICS FOR 10:\n",
      "{'NDCG@10': 0.58982281741011, 'Recall@10': 0.5233506631763168, 'MRR@10': 0.7337179924136445}\n"
     ]
    }
   ],
   "source": [
    "evaluate_recommendations(df=df_for_recs, ks=[3,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad9d71-1c06-4406-9c50-0df2581ee3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
